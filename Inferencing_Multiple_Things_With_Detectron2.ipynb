{"cells":[{"cell_type":"markdown","metadata":{"id":"t_hHGUpgJBF2"},"source":[" <center><font size=\"6\">Welcome to my detectron2 notebook."]},{"cell_type":"markdown","metadata":{"id":"h2aTTH8II3eq"},"source":["I am at the University of Michigan working on a research project in Ye Labs. As part of my responsibilities, I was asked to find a method for removing a 'dynamic' background in a set of videos which include animals, leaving behind only the animals. After searching around for solutions and spending a day *trying* to make the [matterport Mask R-CNN](https://github.com/matterport/Mask_RCNN) code work natively on my Mac M1, I stumbled upon a [youtube video](https://www.youtube.com/watch?v=9a_Z14M-msc) explaining how to work with detectron2. Detectron2 was immediately appealing and made me realize that the problem statement I was working with needed to be adjusted. I mentally changed my assignment to \"given a video with animals in it, return only the animals without any other background content.\" This may sound the same as my original assignment, but to me, the subtle difference in thought process was significant. So I decided to train and use detectron2 as the base for identifying the animals in my videos, creating instance masks, extracting the animals from videos based on these masks, pasting them onto a blank canvas, and then ultimately saving the newly constructed canvas as a video frame in a new video.   \n","\n","This notebook includes only the inference portion of my work. Please see my \"Detecting_Multiple_Things_With_Detectron2\" notebook for detailed explanations of every step from dataset creation, image annotations, training, as well as the inference steps found in here.  I have stripped down this notebook to the bare minimum even excluding comments in code to minimize clutter. "]},{"cell_type":"markdown","source":["# Set up the notebook"],"metadata":{"id":"If8WwVgYwSPj"}},{"cell_type":"code","source":["local = False\n","nvidia_alt ='cpu'"],"metadata":{"id":"4Bp5TO4bZ9Ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKKorwo0y1Nh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672262456356,"user_tz":480,"elapsed":19708,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"0f5e64eb-a0d1-4fe6-8879-1e7ac534f3d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["if not local: \n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Check which versions of pyyaml, torch, and torchvision are installed\n","# If the versions I want are not installed, do it now\n","# If they are installed, don't waste time downloading the wheel\n","!pip install pyyaml\n","torch_version = !pip show torch | grep Version\n","torchvision_version = !pip show torchvision | grep Version\n","if torch_version[0] != 'Version: 1.10.1+cu113':\n","  !pip install torch==1.10.1+cu113 -f \\\n","    https://download.pytorch.org/whl/torch_stable.html\n","else:\n","  print('torch ' + torch_version[0] + ' already installed')\n","if torchvision_version[0] != 'Version: 0.11.2+cu113':\n","  !pip install torchvision==0.11.2+cu113 -f \\\n","    https://download.pytorch.org/whl/torch_stable.html\n","else:\n","  print('torchvision ' + torchvision_version[0] + ' already installed')"],"metadata":{"id":"UscS-bsGCRbu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672262673396,"user_tz":480,"elapsed":209428,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"8283f980-ba23-4f31-dc6b-a15b40b67de2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.10.1+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp38-cp38-linux_x86_64.whl (1821.4 MB)\n","\u001b[K     |██████████████▋                 | 834.1 MB 1.3 MB/s eta 0:12:46tcmalloc: large alloc 1147494400 bytes == 0x6672a000 @  0x7f0a8c7c4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |██████████████████▌             | 1055.7 MB 1.3 MB/s eta 0:10:12tcmalloc: large alloc 1434370048 bytes == 0x3f52000 @  0x7f0a8c7c4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |███████████████████████▌        | 1336.2 MB 1.2 MB/s eta 0:06:33tcmalloc: large alloc 1792966656 bytes == 0x5973e000 @  0x7f0a8c7c4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.2 MB/s eta 0:01:48tcmalloc: large alloc 2241208320 bytes == 0xc4526000 @  0x7f0a8c7c4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |████████████████████████████████| 1821.4 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1821442048 bytes == 0x3f52000 @  0x7f0a8c7c31e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n","tcmalloc: large alloc 2276802560 bytes == 0x149e88000 @  0x7f0a8c7c4615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91 0x5d8941 0x4fe318\n","\u001b[K     |████████████████████████████████| 1821.4 MB 8.4 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1+cu113) (4.4.0)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.1+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torchvision==0.11.2+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.2%2Bcu113-cp38-cp38-linux_x86_64.whl (24.6 MB)\n","\u001b[K     |████████████████████████████████| 24.6 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (7.1.2)\n","Requirement already satisfied: torch==1.10.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2+cu113) (1.10.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1->torchvision==0.11.2+cu113) (4.4.0)\n","Installing collected packages: torchvision\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.0+cu116\n","    Uninstalling torchvision-0.14.0+cu116:\n","      Successfully uninstalled torchvision-0.14.0+cu116\n","Successfully installed torchvision-0.11.2+cu113\n"]}]},{"cell_type":"code","source":["# Check which version of detectron2 is installed.\n","# If the version I want is not installed, do it now\n","# If it is installed, do not waste time downloading the wheel \n","detectron2_version = !pip show detectron2 | grep Version\n","if detectron2_version[0] != 'Version: 0.6+cu113':\n","  !python -m pip install detectron2==0.6 -f \\\n","    https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n","else:\n","  print('detectron2 ' + detectron2_version[0] + ' already installed')"],"metadata":{"id":"2idOY556Exu8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672262718665,"user_tz":480,"elapsed":16375,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"d119413c-0166-4124-c205-151043a74f6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n","Collecting detectron2==0.6\n","  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/detectron2-0.6%2Bcu113-cp38-cp38-linux_x86_64.whl (7.0 MB)\n","\u001b[K     |████████████████████████████████| 7.0 MB 737 kB/s \n","\u001b[?25hCollecting fvcore<0.1.6,>=0.1.5\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 4.0 MB/s \n","\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (1.5.0)\n","Collecting black==21.4b2\n","  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n","\u001b[K     |████████████████████████████████| 130 kB 17.4 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (3.2.2)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (1.3.0)\n","Collecting yacs>=0.1.8\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (2.9.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (0.16.0)\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (7.1.2)\n","Collecting omegaconf>=2.1\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.7 MB/s \n","\u001b[?25hCollecting hydra-core>=1.1\n","  Downloading hydra_core-1.3.1-py3-none-any.whl (154 kB)\n","\u001b[K     |████████████████████████████████| 154 kB 23.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (4.64.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (2.1.1)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (2.0.6)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from detectron2==0.6) (0.8.10)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.8/dist-packages (from black==21.4b2->detectron2==0.6) (1.4.4)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n","Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from black==21.4b2->detectron2==0.6) (0.10.2)\n","Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from black==21.4b2->detectron2==0.6) (7.1.2)\n","Collecting pathspec<1,>=0.8.1\n","  Downloading pathspec-0.10.3-py3-none-any.whl (29 kB)\n","Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.8/dist-packages (from black==21.4b2->detectron2==0.6) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from hydra-core>=1.1->detectron2==0.6) (21.3)\n","Collecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 31.9 MB/s \n","\u001b[?25hCollecting portalocker\n","  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.11.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (57.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (1.3.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (1.51.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (2.15.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (0.38.4)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.6) (3.19.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (5.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.12.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.2)\n","Building wheels for collected packages: fvcore, antlr4-python3-runtime\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61431 sha256=b8b02b456fae3f392a64a556a3a0c5003c4f3c3815e89ed18a549db8d21adf2c\n","  Stored in directory: /root/.cache/pip/wheels/b8/79/07/c0e9367f5b5ea325e246bd73651e8af175fabbef943043b1cc\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=a157cb9bbf07cf6d82aef92e45c5c9481df52a3769710931cc73b1b00333e8ce\n","  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n","Successfully built fvcore antlr4-python3-runtime\n","Installing collected packages: portalocker, antlr4-python3-runtime, yacs, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n","Successfully installed antlr4-python3-runtime-4.9.3 black-21.4b2 detectron2-0.6+cu113 fvcore-0.1.5.post20221221 hydra-core-1.3.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.3.0 pathspec-0.10.3 portalocker-2.6.0 yacs-0.1.8\n"]}]},{"cell_type":"markdown","metadata":{"id":"GDTVSBBBfePS"},"source":["## Get Info About Your Environment\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2SV2sGLfnmb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672262754332,"user_tz":480,"elapsed":1049,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"667db3d4-dec4-41c1-8b4b-eab1b974ee8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-0c8c7ac8-a10c-244c-941f-fee352fcf45b)\n","Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\n"]}],"source":["# If you are local, I assume you know your resources; Therefore, it does not\n","# run when you are operating locally\n","if not local:\n","    !nvidia-smi -L\n","    !lscpu |grep 'Model name'"]},{"cell_type":"markdown","metadata":{"id":"5Zdan5fRJfGP"},"source":["#Import The Required Libraries\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVw4T1X1DXts"},"outputs":[],"source":["import os\n","import torch \n","import cv2\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime \n","from tqdm.notebook import tqdm_notebook\n","from detectron2.utils.visualizer import ColorMode\n","from detectron2.utils.video_visualizer import VideoVisualizer\n","\n","%matplotlib inline\n","\n","\n","if local:\n","  from tqdm import tqdm as tqdm_notebook\n","else: \n","  from tqdm.notebook import tqdm_notebook\n","  from google.colab.patches import cv2_imshow\n","  from google.colab import runtime\n","\n","\n","\n","if torch.cuda.is_available():\n","  device = 'cuda'\n","else:\n","  device = nvidia_alt"]},{"cell_type":"markdown","metadata":{"id":"fFFZrtVxgH8x"},"source":["#Set up Your Global Variables Here. <font color='red'>Run Every Time!</font>\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMw1OpypMAlK"},"outputs":[],"source":["my_dataset_path = '/content/drive/MyDrive/datasets/'\n","my_things = ['rat', 'larva',]\n","my_display_things = ['rat','larva',]\n","my_prediction_threshold = 0.75\n","my_dataset_name = 'combo'\n","image_extensions = ['.jpg','.png',]\n","video_extensions = ['.mp4','.mov', '.avi',]\n","disconnect_on_complete = True"]},{"cell_type":"code","source":["if not local:\n","  import sys\n","  sys.path.append(my_dataset_path + my_dataset_name + '/sys/')\n","from thing_masker import thing_masker "],"metadata":{"id":"iihoFc8Iwmu0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90kwQnMOGfbw"},"source":["# Test Your Model on Images!"]},{"cell_type":"code","source":["thing = thing_masker(my_things, my_dataset_name,\\\n","                     my_dataset_path) \n","my_dataset_metadata = thing.metadata"],"metadata":{"id":"5TekJ_wxAghM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__YlqFrK0DyK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672262861225,"user_tz":480,"elapsed":1493,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"4d951e3d-3fc2-4dd8-abb3-1e711d88ec80"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['rat', 'larva']"]},"metadata":{},"execution_count":11}],"source":["temp_weights = my_dataset_path + my_dataset_name + \\\n","        '/models/' + my_dataset_name + '_model_final.pth'\n","temp_config=None\n","thing.init_predictor(my_prediction_threshold, device, \n","                     temp_weights, temp_config)\n","thing.metadata.thing_classes"]},{"cell_type":"markdown","metadata":{"id":"Qx-h_emGr4kW"},"source":["###Process some images and display results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxbgncjFuwN6","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uVZbD-OSA0wzcs1XENXaEDJDeFG75FUC"},"executionInfo":{"status":"ok","timestamp":1672262903535,"user_tz":480,"elapsed":17113,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"3c76def8-9ded-4ce0-8b63-eb100456f649"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["test_images = thing.get_test_files(image_extensions, thing.test_directory)\n","for d in random.sample(test_images, 6):\n","  image_from_file = cv2.imread(d)\n","  thing.update_outputs(image_from_file)\n","  masked_image = thing.get_masked_image(image_from_file, my_display_things)\n","  original_with_masks = thing.get_original_with_masks(image_from_file,\n","                                                      my_display_things)\n","  print(d)\n","  fig = plt.figure(figsize=(30, 30))\n","  fig.add_subplot(1, 2, 1)\n","  plt.imshow(masked_image)\n","  plt.axis('off')\n","  plt.title(\"Masked\")\n","  fig.add_subplot(1,2,2)\n","  plt.imshow(cv2.cvtColor(original_with_masks.get_image()[:, :, ::-1],\\\n","                          cv2.COLOR_BGR2RGB))\n","  plt.axis('off')\n","  plt.title(\"Original With Mask\")\n","  plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"kwpO-U2LAVqu"},"source":["#Process a Video"]},{"cell_type":"markdown","metadata":{"id":"LePNaj4eOavy"},"source":["##Start Working with Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTK9du-KAaLd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672262988263,"user_tz":480,"elapsed":219,"user":{"displayName":"Isabelle Baker","userId":"02804340398657943647"}},"outputId":"6e3ae178-04e1-4b61-a55b-ed51c2a1891b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/datasets/combo/videos/Video_Of_Larva_1.mp4',\n"," '/content/drive/MyDrive/datasets/combo/videos/Video_Of_Rodent_1.mp4',\n"," '/content/drive/MyDrive/datasets/combo/videos/Video_Of_Rodent_2.mp4',\n"," '/content/drive/MyDrive/datasets/combo/videos/Video_Of_Rat_3.mp4']"]},"metadata":{},"execution_count":13}],"source":["test_videos = thing.get_test_files(video_extensions, thing.video_test_directory)\n","test_videos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QmGh7G7BAae","colab":{"base_uri":"https://localhost:8080/","height":794,"referenced_widgets":["1893e34f2ec649a39bd072e8791ab386","2a5b185238b04b108c0e0c4a20365e0b","25d71bad798c4281926c8d9acec28cf7","bdc5fe3537b94bd180e595e18c1b3535","3d00ac80dca64061825f03acaf41b729","543fc0de3f6142e3b90f6c34b1ff8a50","a98f5b26e804479eb43a3311c5f0c730","52621af901394b0fb4ea1fef9bbcd2e6","f58da70b480741769117d80b2bb23562","5d1cb594920e48d6b281420b1ec7774c","57451e015cf341f48d1024cebfefe8d2"]},"outputId":"f67d01cb-b7d6-49d6-c16e-fde576ae3776"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/datasets/combo/videos/Video_Of_Rodent_1.mp4\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|                                                                       | 0/516 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1893e34f2ec649a39bd072e8791ab386"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["done saving videos\n","converting original with mask to save space...\n","ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Input #0, avi, from '/content/drive/MyDrive/datasets/combo/videos_output/original_showing_masks_Video_Of_Rodent_1_2022-12-28_21:29:57.242274_temp.avi':\n","  Metadata:\n","    encoder         : Lavf58.76.100\n","  Duration: 00:00:20.64, start: 0.000000, bitrate: 228201 kb/s\n","    Stream #0:0: Video: mjpeg (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 3840x2160, 228603 kb/s, 25 fps, 25 tbr, 25 tbn, 25 tbc\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n","Press [q] to stop, [?] for help\n","\u001b[1;36m[libx264 @ 0x557b91c01e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n","\u001b[1;36m[libx264 @ 0x557b91c01e00] \u001b[0mprofile High, level 5.1\n","\u001b[1;36m[libx264 @ 0x557b91c01e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=5 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=8 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=2 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=3 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=50 rc=crf mbtree=1 crf=35.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n","Output #0, mp4, to '/content/drive/MyDrive/datasets/combo/videos_output/original_showing_masks_Video_Of_Rodent_1_2022-12-28_21:29:57.242274_result.mp4':\n","  Metadata:\n","    encoder         : Lavf57.83.100\n","    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 3840x2160, q=-1--1, 25 fps, 12800 tbn, 25 tbc\n","    Metadata:\n","      encoder         : Lavc57.107.100 libx264\n","    Side data:\n","      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n"]}],"source":["test_video_name = random.sample(test_videos, 4)\n","for video in test_video_name:\n","  print(video)\n","  cap = cv2.VideoCapture(video)\n","  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","  frames_per_second = cap.get(cv2.CAP_PROP_FPS)\n","  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","  date_time = str(datetime.now()).replace(\" \", \"_\")\n","  masked_images_video_result = thing.video_output_directory + \"masked_images_\" \\\n","                                + os.path.split(video)[1][:-4]+ '_'+\\\n","                                date_time + \"_result.avi\"\n","  original_showing_masks_temp = thing.video_output_directory + \\\n","    \"original_showing_masks_\" + os.path.split(video)[1][:-4] + '_'+\\\n","    date_time + \"_temp.avi\" \n","  original_showing_masks_result = thing.video_output_directory + \\\n","    \"original_showing_masks_\" + os.path.split(video)[1][:-4] + '_'+\\\n","    date_time + \"_result.mp4\" \n","  fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n","  output_writer1 = cv2.VideoWriter(masked_images_video_result, fourcc, \n","                                  frames_per_second, (width, height))\n","  output_writer2 = cv2.VideoWriter(original_showing_masks_temp, fourcc, \n","                                  frames_per_second, (width, height))\n","  if num_frames == 0:\n","      cap.release()\n","      assert num_frames == 0, 'video not found or empty!'\n","  video_vis = VideoVisualizer(thing.metadata, ColorMode.IMAGE)\n","  video_length=int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","  for frame_count in tqdm_notebook(range(video_length), ncols=100):\n","      _, frame = cap.read()\n","      if frame is None:\n","          print('end of input file reached')\n","          cap.release()\n","          cv2.destroyAllWindows()\n","          break\n","      thing.update_outputs(frame)\n","      foreground = thing.get_masked_image(frame, my_display_things)\n","      frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","      video_filter = thing.outputs['instances']\n","      i=0\n","      for i in range(len(thing.classes)):\n","        if not thing.classes[i] in my_display_things:\n","              video_filter=video_filter[video_filter.pred_classes != i]\n","      visualization = video_vis.draw_instance_predictions(frame, \n","                            video_filter.to('cpu'))\n","      visualization = cv2.cvtColor(visualization.get_image(), cv2.COLOR_RGB2BGR)\n","      output_writer1.write(np.array(foreground))\n","      output_writer2.write(visualization)\n","  output_writer1.release()\n","  output_writer2.release()\n","  print('done saving videos')\n","  cap.release()\n","  print('converting original with mask to save space...')\n","  input_file = original_showing_masks_temp\n","  output_file = original_showing_masks_result\n","  !ffmpeg -i \"{input_file}\" -c:v libx264 \\\n","      -preset slow -crf 35 -c:a copy \"{output_file}\"\n","  #if os.path.exists(original_showing_masks_temp):\n","  #  os.remove(original_showing_masks_temp)\n","  #else:\n","  #  print(\"The file does not exist\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0x-R9q8rYBW"},"outputs":[],"source":["if (not local) and (disconnect_on_complete): runtime.unassign() "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"10bBMwcVFdkX3UuEMhpA9kI-UpPgxuWbh","timestamp":1671912131721},{"file_id":"1RXhRPTATH-ZpYZZTcsmsbTlngHiIMlUr","timestamp":1670875720408}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1893e34f2ec649a39bd072e8791ab386":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a5b185238b04b108c0e0c4a20365e0b","IPY_MODEL_25d71bad798c4281926c8d9acec28cf7","IPY_MODEL_bdc5fe3537b94bd180e595e18c1b3535"],"layout":"IPY_MODEL_3d00ac80dca64061825f03acaf41b729"}},"2a5b185238b04b108c0e0c4a20365e0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_543fc0de3f6142e3b90f6c34b1ff8a50","placeholder":"​","style":"IPY_MODEL_a98f5b26e804479eb43a3311c5f0c730","value":"100%"}},"25d71bad798c4281926c8d9acec28cf7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52621af901394b0fb4ea1fef9bbcd2e6","max":516,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f58da70b480741769117d80b2bb23562","value":516}},"bdc5fe3537b94bd180e595e18c1b3535":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d1cb594920e48d6b281420b1ec7774c","placeholder":"​","style":"IPY_MODEL_57451e015cf341f48d1024cebfefe8d2","value":" 516/516 [12:31&lt;00:00,  1.35s/it]"}},"3d00ac80dca64061825f03acaf41b729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100px"}},"543fc0de3f6142e3b90f6c34b1ff8a50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a98f5b26e804479eb43a3311c5f0c730":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52621af901394b0fb4ea1fef9bbcd2e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f58da70b480741769117d80b2bb23562":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d1cb594920e48d6b281420b1ec7774c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57451e015cf341f48d1024cebfefe8d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}